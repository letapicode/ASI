numpy
torch
# optional: flash-attn for FlashAttention-3 support
